# IKF Market Pipeline – Code Catalog

_Branch: **main** | Files: **25**_

## Index

- `CATALOG.md`  (49 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/CATALOG.md)
- `config/composites_specs/IKF_AI_Megacap.csv`  (75 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_AI_Megacap.csv)
- `config/composites_specs/IKF_EnergyPulse.csv`  (59 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_EnergyPulse.csv)
- `config/config.yaml`  (3 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/config.yaml)
- `modules/__init__.py`  (0 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/__init__.py)
- `modules/composites.py`  (7 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/composites.py)
- `modules/equities.py`  (4 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/equities.py)
- `modules/features.py`  (10 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features.py)
- `modules/features_dedup.py`  (4 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features_dedup.py)
- `modules/futures.py`  (2 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/futures.py)
- `modules/options.py`  (1 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/options.py)
- `modules/perf.py`  (4 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/perf.py)
- `modules/plotting.py`  (5 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/plotting.py)
- `modules/ranking.py`  (9 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/ranking.py)
- `modules/rates.py`  (1 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/rates.py)
- `modules/report_equities.py`  (2 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/report_equities.py)
- `modules/reporting.py`  (4 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/reporting.py)
- `quick_check.py`  (714 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/quick_check.py)
- `Readme.md`  (6 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/Readme.md)
- `requirements.txt`  (144 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/requirements.txt)
- `run_pipeline.py`  (15 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/run_pipeline.py)
- `utils/__init__.py`  (0 B, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/__init__.py)
- `utils/build_catalog.py`  (2 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/build_catalog.py)
- `utils/build_navs_from_prices.py`  (10 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/build_navs_from_prices.py)
- `utils/helpers.py`  (2 KB, modified 2025-11-12 09:18:12Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/helpers.py)

---

## File Previews


### `CATALOG.md`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/CATALOG.md)

```md
# IKF Market Pipeline – Code Catalog

_Branch: **main** | Files: **21**_

## Index

- `CATALOG.md`  (49 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/CATALOG.md)
- `config/composites_specs/IKF_AI_Megacap.csv`  (75 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_AI_Megacap.csv)
- `config/composites_specs/IKF_EnergyPulse.csv`  (59 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_EnergyPulse.csv)
- `config/config.yaml`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/config.yaml)
- `modules/__init__.py`  (0 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/__init__.py)
- `modules/composites.py`  (7 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/composites.py)
- `modules/equities.py`  (4 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/equities.py)
- `modules/features.py`  (10 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features.py)
- `modules/features_dedup.py`  (4 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features_dedup.py)
- `modules/futures.py`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/futures.py)
- `modules/options.py`  (1 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/options.py)
- `modules/ranking.py`  (9 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/ranking.py)
- `modules/rates.py`  (1 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/rates.py)
- `modules/report_equities.py`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/report_equities.py)
- `quick_check.py`  (714 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/quick_check.py)
- `Readme.md`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/Readme.md)
- `requirements.txt`  (144 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/requirements.txt)
- `run_pipeline.py`  (6 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/run_pipeline.py)
- `utils/__init__.py`  (0 B, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/__init__.py)
- `utils/build_catalog.py`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/build_catalog.py)
- `utils/helpers.py`  (2 KB, modified 2025-11-11 20:21:19Z UTC) → [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/helpers.py)

---

## File Previews


### `CATALOG.md`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/CATALOG.md)

```md
# IKF Market Pipeline – Code Catalog

_Branch: **main** | Files: **21**_

## Index

- `CATALOG.md`  (42 KB, mod
...
[truncated]
```

### `config/composites_specs/IKF_AI_Megacap.csv`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_AI_Megacap.csv)

```csv
ticker,weight
NVDA,0.22
MSFT,0.20
AAPL,0.16
GOOGL,0.16
META,0.13
AMZN,0.13

```

### `config/composites_specs/IKF_EnergyPulse.csv`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/composites_specs/IKF_EnergyPulse.csv)

```csv
ticker,weight
XOM,0.25
CVX,0.25
SLB,0.20
COP,0.15
EOG,0.15

```

### `config/config.yaml`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/config/config.yaml)

```yaml
# FILE: config/config.yaml
# IKF MARKET PIPELINE — CONFIG

storage:
  root: "data/raw"
  format: "parquet"
  timezone: "US/Eastern"

defaults:
  history_start: "2010-01-01"

equities:
  enabled: true
  universe: ["SPY"]         # runtime auto-adds composite members + benchmarks
  price_interval: "1d"
  mode: "incremental"       # overridden by --recent/--full
  lookback_days: 365
  overlap_days: 5
  max_attempts: 3
  base_sleep: 1.5
  between_sleep: 0.35

options:
  enabled: true
  underlying: ["SPY"]
  expires: "nearest_3"
  chains: ["calls","puts"]

futures:
  enabled: true
  tickers: ["ES=F"]
  price_interval: "1d"
  history_start: "2010-01-01"
  overlap_days: 5

rates:
  enabled: true
  fred_series: ["SOFR","DGS2","DGS5","DGS10","DGS30"]
  start: "2010-01-01"
  overlap_days: 5

features:
  equities:
    enabled: true
    benchmark: "SPY"
    win_ret: 20
    win_vol: 60
    win_sharpe: 60
    sma_windows: [20, 50]
    ema_windows: [20, 50]
    rsi_period: 14
    boll_window: 20
    atr_window: 14
    skew_window: 60
    kurt_window: 60
    plots: true
    processed_path: "data/processed"
    plots_path: "output/plots"

analysis:
  benchmark: "SPY"
  topk: 25
  deciles: 10

# IKF COMPOSITES (CSV > inline > equal-weight)
composites:
  - name: "IKF_AI_Megacap"
    tickers: ["NVDA","MSFT","AAPL","GOOGL","META","AMZN"]
    weights_csv: "config/composites_specs/IKF_AI_Megacap.csv"   # ← UNCOMMENT / add this
    # weights: [0.22, 0.20, 0.16, 0.16, 0.13, 0.13]             # ← optional to leave, ignored when CSV present
    benchmark: "SPY"
    rebalance: "none"
    start: "2010-01-01"

  - name: "IKF_EnergyPulse"
    tickers: ["XOM","CVX","SLB","COP","EOG"]
    weights_csv: "config/composites_specs/IKF_EnergyPulse.csv"  # ← UNCOMMENT / add this
    # weights: [0.25, 0.25, 0.20, 0.15, 0.15]                  # ← optional to leave, ignored when CSV present
    benchmark: "SPY"
    rebalance: "none"
    start: "2010-01-01"

plot:
  style: seaborn-whitegrid
  dpi: 140
  colors:
    spy: "#9E9E9E"        # grey benchmark
    ikf_green: "#1BAA4A"  # IKF green
    accent: "#3B82F6"
  marker:
    size_normal: 60
    size_highlight: 110
  line:
    width_normal: 2.0
    width_highlight: 3.0

report:
  top_n: 10
  include_sections:
    - leaders_laggards
    - cum_return
    - rolling_sharpe
    - drawdown
    - risk_return
    - corr_heatmap
  # NEW: IKF-style chart windows (order matters)
  ikf_windows: ["3d", "7d", "14d", "1m", "3m", "1y"]


backtest:
  rolling_sharpe_window_days: 60
  rebal_freq: "monthly"     # none|monthly|quarterly

paths:
  plots: "output/plots"
  reports: "output/reports"
  snapshots: "output/snapshots"
  processed: "data/processed"

```

### `modules/__init__.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/__init__.py)

```py

```

### `modules/composites.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/composites.py)

```py
# FILE: modules/composites.py
from __future__ import annotations
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, List, Dict
import pandas as pd
import numpy as np

# expects utils.helpers to provide these (already in your repo)
from utils.helpers import parquet_path, load_parquet

"""
Composites builder
- Builds composite price time series from RAW equities parquet (no feature dependency)
- IKF weights via inline `weights:` or `weights_csv:` (CSV wins if both supplied)
- Rebalancing: "none" | "monthly" | "quarterly" (default "none")
- Writes data/processed/composites/<name>_prices.parquet
- Optionally writes output/snapshots/<name>_snapshot.csv if rank_today provided
CSV format for weights:
    ticker,weight
    NVDA,0.22
    MSFT,0.20
"""

@dataclass
class CompositeSpec:
    name: str
    tickers: List[str]
    weights: Optional[List[float]] = None
    weights_csv: Optional[str] = None
    benchmark: Optional[str] = None
    rebalance: str = "none"
    start: Optional[str] = None

def _ensure_cols(df: pd.DataFrame, cols: List[str]):
    miss = [c for c in cols if c not in df.columns]
    if miss: raise KeyError(f"Missing columns: {miss}")

def _read_prices_from_raw(storage_root: str, tickers: List[str]) -> pd.DataFrame:
    out = []
    for t in tickers:
        p = parquet_path(storage_root, "equities", t)
        df = load_parquet(p)
        if df.empty: continue
        cols = {str(c).lower(): c for c in df.columns}
        close_col = None
        for cand in ("adj close", "close"):
            if cand in cols: close_col = cols[cand]; break
        if close_col is None: continue
        s = pd.to_numeric(df[close_col].squeeze(), errors="coerce")
        s.index = pd.to_datetime(s.index); s.name = "close"
        tmp = s.reset_index().rename(columns={s.index.name or "index": "date"})
        tmp["ticker"] = t.upper()
        out.append(tmp[["date","ticker","close"]])
    if not out:
        return pd.DataFrame(columns=["date","ticker","close"])
    dfall = pd.concat(out, ignore_index=True)
    return dfall.sort_values(["ticker","date"]).dropna(subset=["close"])

def _normalize_weights(tickers: List[str], weights: Optional[List[float]], weights_csv: Optional[str]) -> pd.DataFrame:
    """Priority: weights_csv > inline weights > equal-weight fallback."""
    if weights_csv:
        wdf = pd.read_csv(weights_csv)
        _ensure_cols(wdf, ["ticker","weight"])
        wdf["ticker"] = wdf["ticker"].astype(str).str.upper()
        wdf = wdf[wdf["ticker"].isin([t.upper() for t in tickers])].copy()
        if wdf.empty: raise ValueError(f"weights_csv={weights_csv} has no rows for declared tickers.")
        w = wdf["weight"].astype(float).values
        s = np.nansum(w)
        if not np.isfinite(s) or s <= 0: raise ValueError("Invalid weights in CSV; sum must be > 0.")
        wdf["weight"] = wdf["weight"] / s
        return wdf[["ticker","weight"]]
    if weights is not None:
        if len(weights) != len(tickers): raise ValueError("Inline 'weights' length must match 'tickers'.")
        w = np.asarray(weights, dtype=float)
        if np.any(~np.isfinite(w)) or w.sum() <= 0: raise ValueError("Inline weights must be finite and sum>0.")
        w = w / w.sum()
        return pd.DataFrame({"ticker":[t.upper() for t in tickers], "weight": w})
    n = len(tickers)
    if n == 0: raise ValueError("No tickers provided.")
    eq = np.repeat(1.0/n, n)
    return pd.DataFrame({"ticker":[t.upper() for t in tickers], "weight": eq})

def _period_starts(dates: pd.DatetimeIndex, freq: str) -> pd.Series:
    if freq == "M":
        lab = pd.Series(dates.to_period("M").astype(str), index=dates)
    elif freq == "Q":
        lab = pd.Series(dates.to_period("Q").astype(str), index=dates)
    else:
        return pd.Series(False, index=dates)
    return lab.ne(lab.shift(1)).fillna(True)

def _build_composite_series(prices: pd.DataFrame, spec: CompositeSpec) -> pd.DataFrame:
    _ensure_cols(prices, ["date","
...
[truncated]
```

### `modules/equities.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/equities.py)

```py
from pathlib import Path
import time
import pandas as pd
import yfinance as yf
from utils.helpers import (
    parquet_path, incremental_append, load_parquet, compute_fetch_start
)

def _mode_start(mode: str,
                exist_df: pd.DataFrame | None,
                history_start: str,
                lookback_days: int,
                overlap_days: int) -> str:
    """
    Decide start date based on mode:
      - full:        history_start
      - recent:      today - lookback_days
      - incremental: compute_fetch_start(existing_index, history_start, overlap_days)
    Always returns an ISO date string.
    """
    mode = (mode or "incremental").lower()
    today = pd.Timestamp.today().normalize()

    if mode == "full":
        return pd.Timestamp(history_start).date().isoformat()

    if mode == "recent":
        start = (today - pd.Timedelta(days=lookback_days)).normalize()
        return start.date().isoformat()

    # incremental (default): small overlap window ending today
    exist_idx = None if exist_df is None or exist_df.empty else exist_df.index
    return compute_fetch_start(exist_idx, history_start=history_start, overlap_days=overlap_days)

def update_equities(cfg: dict, storage_root: str):
    if not cfg.get("enabled", False):
        print("[INFO] equities disabled in config.")
        return

    tickers        = cfg["universe"]
    interval       = cfg.get("price_interval", "1d")
    history_start  = cfg.get("history_start", "2010-01-01")
    mode           = cfg.get("mode", "incremental")
    lookback_days  = int(cfg.get("lookback_days", 365))
    overlap_days   = int(cfg.get("overlap_days", 5))

    # gentle retry knobs
    max_attempts   = int(cfg.get("max_attempts", 3))
    base_sleep     = float(cfg.get("base_sleep", 1.5))
    between_sleep  = float(cfg.get("between_sleep", 0.35))

    print(f"[INFO] Equities mode={mode} interval={interval} "
          f"(history_start={history_start}, lookback_days={lookback_days}, overlap_days={overlap_days})")
    print(f"[INFO] Rate limit: attempts={max_attempts}, base_sleep={base_sleep}s, between={between_sleep}s")

    for t in tickers:
        path  = parquet_path(storage_root, "equities", t)
        exist = load_parquet(path)

        start = _mode_start(
            mode=mode,
            exist_df=exist,
            history_start=history_start,
            lookback_days=lookback_days,
            overlap_days=overlap_days
        )

        df = None
        err = None
        for attempt in range(1, max_attempts + 1):
            try:
                df = yf.download(
                    t, start=start, interval=interval,
                    auto_adjust=True, progress=False
                )
                break
            except Exception as e:
                err = e
                time.sleep(base_sleep * attempt)

        if df is None or df.empty:
            if err:
                print(f"[WARN] {t}: download error after {max_attempts} attempts: {err}")
            else:
                print(f"[INFO] {t}: no rows returned for start={start}")
            time.sleep(between_sleep)
            continue

        # tidy columns/index
        if isinstance(df.columns, pd.MultiIndex):
            df.columns = df.columns.get_level_values(0)
        df.columns = [str(c).strip().title() for c in df.columns]
        df.index = pd.to_datetime(df.index); df.index.name = "Date"

        # upsert (concat/sort/dedup) handled inside incremental_append
        incremental_append(df, path, index_name="Date")
        print(f"[OK] {t}: mode={mode} window={start}..today rows={len(df):,} -> {path}")

        time.sleep(between_sleep)

```

### `modules/features.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features.py)

```py
from pathlib import Path
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from ta.momentum import RSIIndicator
from ta.trend import SMAIndicator, EMAIndicator, MACD
from utils.helpers import ensure_dir

RAW_EQUITIES_DIR = Path("data/raw/equities")

# ---------- helpers ----------
def _rolling_beta_alpha(ret_stock: pd.Series, ret_bench: pd.Series, win: int = 60):
    both = pd.concat([ret_stock, ret_bench], axis=1).dropna()
    both.columns = ["s", "m"]
    cov = both["s"].rolling(win).cov(both["m"])
    var = both["m"].rolling(win).var()
    beta = cov / var
    alpha = (both["s"].rolling(win).mean() - beta * both["m"].rolling(win).mean()) * 252
    return beta, alpha

def _drawdown(px: pd.Series):
    roll_max = px.cummax()
    dd = px / roll_max - 1.0
    mdd = dd.min()
    return dd, mdd

def _atr(high: pd.Series, low: pd.Series, close: pd.Series, window: int = 14):
    hl = (high - low).abs()
    hc = (high - close.shift(1)).abs()
    lc = (low - close.shift(1)).abs()
    tr = pd.concat([hl, hc, lc], axis=1).max(axis=1)
    return tr.rolling(window).mean()

def _bollinger(px: pd.Series, window: int = 20, k: float = 2.0):
    ma = px.rolling(window).mean()
    sd = px.rolling(window).std()
    upper = ma + k * sd
    lower = ma - k * sd
    pct_band = (px - lower) / (upper - lower)  # 0..1 position in band
    return ma, upper, lower, pct_band

def _safe_to_datetime_idx(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.index = pd.to_datetime(df.index)
    df.sort_index(inplace=True)
    return df

def _to_series(df: pd.DataFrame, col: str) -> pd.Series:
    canon = {c: str(c).strip() for c in df.columns}
    df = df.rename(columns=canon)
    if col not in df.columns:
        alt = {c.lower(): c for c in df.columns}
        if col.lower() in alt:
            col = alt[col.lower()]
    s = df.get(col, None)
    if s is None:
        return pd.Series(dtype="float64", index=df.index)
    if isinstance(s, pd.DataFrame):
        s = s.iloc[:, 0]
    if not isinstance(s, pd.Series):
        s = pd.Series(s, index=df.index)
    s = pd.to_numeric(s, errors="coerce"); s.name = col
    return s

def _compute_one_ticker_features(df: pd.DataFrame, ticker: str, cfg: dict) -> pd.DataFrame:
    """Build per-ticker feature frame. RETURNS empty df if no usable price series."""
    if df.empty:
        return pd.DataFrame()

    # normalize index & columns
    df = _safe_to_datetime_idx(df)
    if isinstance(df.columns, pd.MultiIndex):               # flatten if needed
        df.columns = df.columns.get_level_values(0)
    df.columns = [str(c).strip() for c in df.columns]

    # config
    WIN_RET     = int(cfg.get("win_ret", 20))
    WIN_VOL     = int(cfg.get("win_vol", 60))
    WIN_SHARPE  = int(cfg.get("win_sharpe", 60))
    SMA_WINDOWS = list(cfg.get("sma_windows", [20, 50]))
    EMA_WINDOWS = list(cfg.get("ema_windows", [20, 50]))
    RSI_PERIOD  = int(cfg.get("rsi_period", 14))
    BOLL_W      = int(cfg.get("boll_window", 20))
    ATR_W       = int(cfg.get("atr_window", 14))
    SKEW_W      = int(cfg.get("skew_window", 60))
    KURT_W      = int(cfg.get("kurt_window", 60))

    # robust close selection
    candidates = ["Adj Close", "Adj close", "close", "Close"]
    px = None
    for c in candidates:
        if c in df.columns:
            s = df[c]
            if isinstance(s, pd.DataFrame):
                s = s.iloc[:, 0]
            px = pd.to_numeric(s, errors="coerce")
            break
    if px is None or px.size == 0:
        lower = {str(c).lower(): c for c in df.columns}
        for want in ["adj close", "close"]:
            if want in lower:
                s = df[lower[want]]
                if isinstance(s, pd.DataFrame):
                    s = s.iloc[:, 0]
                px = pd.to_numeric(s, errors="coerce")
                break
    if px is None or px.size == 0:
        return pd.DataFrame()

    px = px.reindex(df.index)  # align

    # base features
    ret
...
[truncated]
```

### `modules/features_dedup.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/features_dedup.py)

```py
# modules/features_dedup.py
from __future__ import annotations

from pathlib import Path
import pandas as pd
import numpy as np

def _ensure_date_column(df: pd.DataFrame, target: str = "date") -> pd.DataFrame:
    """
    Make sure a 'date' column exists.
    - If index is DatetimeIndex, bring it out as a column.
    - Else, search for a datetime-like column and rename it to 'date'.
    """
    if target in df.columns:
        # normalize to pandas datetime (no timezone)
        df[target] = pd.to_datetime(df[target], errors="coerce", utc=True).dt.tz_convert(None)
        return df

    # 1) DatetimeIndex → column
    if isinstance(df.index, pd.DatetimeIndex):
        name = df.index.name if df.index.name else "index"
        df = df.reset_index().rename(columns={name: target})
        df[target] = pd.to_datetime(df[target], errors="coerce", utc=True).dt.tz_convert(None)
        return df

    # 2) Look for obvious names
    candidates = [c for c in df.columns if str(c).lower() in {"date", "datetime", "timestamp"}]
    # 3) Or any column that can be parsed as datetime
    if not candidates:
        for c in df.columns:
            try:
                tmp = pd.to_datetime(df[c], errors="coerce")
                if tmp.notna().mean() > 0.9:  # mostly valid datetimes
                    candidates.append(c)
                    break
            except Exception:
                continue

    if candidates:
        pick = candidates[0]
        if pick != target:
            df = df.rename(columns={pick: target})
        df[target] = pd.to_datetime(df[target], errors="coerce", utc=True).dt.tz_convert(None)
        return df

    raise KeyError(
        "Could not find a date column. Looked for index=DatetimeIndex or columns like "
        "'date'/'datetime'/'timestamp'. Available columns: "
        f"{list(df.columns)[:12]}..."
    )

def _ensure_id_column(df: pd.DataFrame, target: str = "ticker") -> pd.DataFrame:
    """
    Make sure a 'ticker' column exists. Try common alternatives, then normalize.
    """
    if target not in df.columns:
        for alt in ("Ticker", "ticker", "symbol", "Symbol"):
            if alt in df.columns:
                df = df.rename(columns={alt: target})
                break
    if target not in df.columns:
        raise KeyError(f"Missing identifier column '{target}'. Columns: {list(df.columns)[:12]}...")

    df[target] = df[target].astype(str).str.upper()
    return df

def finalize_equity_features_file(
    processed_dir: str = "data/processed",
    fname: str = "equity_features.parquet",
    *,
    date_col: str = "date",
    id_col: str = "ticker",
) -> str:
    """
    Enforce unique primary key (date, ticker) on the engineered features parquet.
    Run this right after feature engineering (and again before ranking as a guard).
    """
    path = Path(processed_dir) / fname
    if not path.exists():
        print(f"[INFO] finalize_equity_features_file: {path} not found, skipping.")
        return str(path)

    df = pd.read_parquet(path)
    n0 = len(df)

    # Ensure keys exist and are normalized
    df = _ensure_date_column(df, target=date_col)
    df = _ensure_id_column(df, target=id_col)

    # Add a deterministic preference column if none exists, so 'keep=last' is meaningful
    prefer_cols = []
    if "ingest_ts" in df.columns: prefer_cols.append("ingest_ts")
    if "calc_ts"   in df.columns: prefer_cols.append("calc_ts")
    if not prefer_cols:
        df["ingest_ts"] = pd.Timestamp.utcnow()
        prefer_cols = ["ingest_ts"]

    # Sort so the last record per (date, ticker) is the one we keep
    sort_cols = [date_col, id_col] + prefer_cols
    df = df.sort_values(sort_cols).drop_duplicates(subset=[date_col, id_col], keep="last")

    # Sanity check
    dup_ct = df.duplicated(subset=[date_col, id_col]).sum()
    assert dup_ct == 0, f"Dedup failed: {dup_ct} duplicate (date,ticker) rows remain."

    # Write back
    df.to_parquet(path)
    n1 = len(df)
    print(f"[OK] Features ded
...
[truncated]
```

### `modules/futures.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/futures.py)

```py
import time
import pandas as pd
import yfinance as yf
from utils.helpers import (
    parquet_path, incremental_append, load_parquet, compute_fetch_start
)

def update_futures(cfg: dict, storage_root: str, default_start="2010-01-01"):
    if not cfg.get("enabled", False):
        return

    tickers       = cfg.get("tickers", [])
    interval      = cfg.get("price_interval", "1d")
    overlap_days  = int(cfg.get("overlap_days", 5))
    history_start = cfg.get("history_start", default_start)

    # reuse gentle knobs if provided
    max_attempts  = int(cfg.get("max_attempts", 3))
    base_sleep    = float(cfg.get("base_sleep", 1.5))
    between_sleep = float(cfg.get("between_sleep", 0.35))

    for t in tickers:
        stem = t.replace("=F","")
        path  = parquet_path(storage_root, "futures", stem)
        exist = load_parquet(path)
        start = compute_fetch_start(
            exist.index if not exist.empty else None,
            history_start=history_start,
            overlap_days=overlap_days
        )

        df = None; err = None
        for attempt in range(1, max_attempts + 1):
            try:
                df = yf.download(
                    t, start=start, interval=interval,
                    auto_adjust=False, progress=False
                )
                break
            except Exception as e:
                err = e
                time.sleep(base_sleep * attempt)

        if df is None or df.empty:
            if err:
                print(f"[WARN] {t}: download error after {max_attempts} attempts: {err}")
            else:
                print(f"[INFO] {t}: no rows returned for start={start}")
            time.sleep(between_sleep)
            continue

        df = df.rename(columns=str.capitalize)
        df.index = pd.to_datetime(df.index); df.index.name = "Date"

        incremental_append(df, path, index_name="Date")
        print(f"[OK] {t}: window={start}..today rows={len(df):,} -> {path}")

        time.sleep(between_sleep)

```

### `modules/options.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/options.py)

```py
import pandas as pd
import yfinance as yf
from utils.helpers import parquet_path, incremental_append

def _nearest_expiries(tkr: yf.Ticker, n: int = 3) -> list[str]:
    exps = tkr.options or []
    return exps[:n]

def update_options(cfg: dict, storage_root: str):
    if not cfg.get("enabled", False):
        return
    underlyings = cfg.get("underlying", [])
    n = int(cfg.get("expiries","nearest_3").split("_")[-1])
    chains = cfg.get("chains", ["calls","puts"])

    for u in underlyings:
        tk = yf.Ticker(u)
        expiries = _nearest_expiries(tk, n)
        for exp in expiries:
            chain = tk.option_chain(exp)
            for side in chains:
                df: pd.DataFrame = getattr(chain, side, None)
                if df is None or df.empty:
                    continue
                df["underlying"] = u
                df["expiry"] = exp
                df.set_index(["expiry","contractSymbol"], inplace=True)
                path = parquet_path(storage_root, "options", u, f"{side}_{exp}")
                incremental_append(df, path)

```

### `modules/perf.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/perf.py)

```py
# modules/perf.py
from __future__ import annotations
import numpy as np
import pandas as pd

TRADING_DAYS = 252

def _to_nav_from_prices(prices: pd.Series) -> pd.Series:
    """Convert a price series to NAV=100 base."""
    s = prices.dropna().astype(float)
    if s.empty: return s
    ret = s.pct_change().fillna(0.0)
    nav = (1.0 + ret).cumprod() * 100.0
    nav.name = getattr(prices, "name", "NAV")
    return nav

def build_composite_nav(prices: pd.DataFrame, weights: pd.DataFrame, rebalance: str) -> pd.Series:
    """
    prices: wide DF (date index, tickers columns) with close prices
    weights: wide DF (date index, tickers columns) with target weights on rebalance dates
    rebalance: "none" | "monthly" | "quarterly"
    Returns NAV=100 pd.Series.
    """
    if prices.empty or weights.empty:
        return pd.Series(dtype=float, name="NAV")

    px = prices.sort_index().ffill().dropna(how="all", axis=1)
    rets = px.pct_change().fillna(0.0)

    if rebalance.lower() == "none":
        # Use first available row of weights and keep static weights (renormalize to 1)
        w0 = weights.iloc[0].reindex(px.columns).fillna(0.0)
        w = (w0 / w0.clip(lower=0).sum()).fillna(0.0)
        port_ret = (rets * w).sum(axis=1)
    else:
        if rebalance.lower() == "monthly":
            rule = "M"
        elif rebalance.lower() == "quarterly":
            rule = "Q"
        else:
            rule = "M"

        w_sched = weights.reindex(px.index).ffill().groupby(pd.Grouper(freq=rule)).head(1)
        w_sched = w_sched.reindex(px.index).ffill().reindex(columns=px.columns).fillna(0.0)
        w_sched = w_sched.div(w_sched.clip(lower=0).sum(axis=1), axis=0).fillna(0.0)
        port_ret = (rets * w_sched).sum(axis=1)

    nav = (1.0 + port_ret).cumprod() * 100.0
    nav.name = "NAV"
    return nav

def _cagr(nav: pd.Series) -> float:
    if nav.empty: return np.nan
    n_days = (nav.index[-1] - nav.index[0]).days
    if n_days <= 0: return np.nan
    total = nav.iloc[-1] / nav.iloc[0]
    years = n_days / 365.25
    return float(total ** (1/years) - 1) if years > 0 else np.nan

def drawdown_series(nav: pd.Series) -> pd.Series:
    if nav.empty: return nav
    peak = nav.cummax()
    dd = nav / peak - 1.0
    dd.name = "drawdown"
    return dd

def rolling_sharpe(returns: pd.Series, window_days: int) -> pd.Series:
    if returns.empty: return returns
    mu = returns.rolling(window_days).mean()
    sd = returns.rolling(window_days).std()
    rs = (mu / sd) * np.sqrt(TRADING_DAYS)
    rs.name = "rolling_sharpe"
    return rs

def _ann_vol(returns: pd.Series) -> float:
    return float(returns.std() * np.sqrt(TRADING_DAYS)) if returns.size else np.nan

def _max_dd(nav: pd.Series) -> float:
    dd = drawdown_series(nav)
    return float(dd.min()) if dd.size else np.nan

def perf_table(nav: pd.Series, spy_nav: pd.Series, asof: pd.Timestamp) -> pd.DataFrame:
    """Return summary metrics for nav vs SPY: YTD, 1Y, Max, Vol, Sharpe, MaxDD, Calmar."""
    def _period_ret(s: pd.Series, start: pd.Timestamp) -> float:
        s = s.loc[s.index >= start]
        return float(s.iloc[-1] / s.iloc[0] - 1.0) if s.size > 1 else np.nan

    idx = pd.to_datetime(nav.index)
    asof = pd.to_datetime(asof) if pd.notna(asof) else idx.max()

    d = {}
    nav_ret = nav.pct_change().fillna(0.0)
    spy_ret = spy_nav.pct_change().fillna(0.0)

    for lbl, days in (("YTD", None), ("1Y", 365), ("MAX", "max")):
        if lbl == "YTD":
            start = pd.Timestamp(year=asof.year, month=1, day=1)
        elif days == "max":
            start = idx.min()
        else:
            start = asof - pd.Timedelta(days=days)
        d[f"ret_{lbl.lower()}"] = _period_ret(nav, start)
    d["vol_ann"] = _ann_vol(nav_ret)
    d["sharpe"] = float((nav_ret.mean() / nav_ret.std()) * np.sqrt(TRADING_DAYS)) if nav_ret.std() > 0 else np.nan
    d["max_dd"] = _max_dd(nav)
    cagr = _cagr(nav)
    d["calmar"] = (cagr / abs(d["max_dd"])) if (pd.notna(cagr) and d["max_dd"]
...
[truncated]
```

### `modules/plotting.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/plotting.py)

```py
# modules/plotting.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Dict, Optional
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

@dataclass
class PlotCfg:
    colors: Dict[str, str]
    dpi: int
    marker: Dict[str, float]
    line: Dict[str, float]
    style: str

def load_plot_cfg(cfg) -> PlotCfg:
    p = cfg.get("plot", {})
    colors = p.get("colors", {}) or {}
    out = PlotCfg(
        colors={
            "spy": colors.get("spy", "#9E9E9E"),       # grey
            "ikf_green": colors.get("ikf_green", "#1BAA4A"),
            "accent": colors.get("accent", "#3B82F6"),
        },
        dpi=int(p.get("dpi", 140)),
        marker=p.get("marker", {"size_normal": 60, "size_highlight": 110}),
        line=p.get("line", {"width_normal": 2.0, "width_highlight": 3.0}),
        style=p.get("style", "seaborn-whitegrid"),
    )
    plt.style.use("seaborn-v0_8-whitegrid" if out.style.startswith("seaborn") else out.style)
    plt.rcParams.update({"figure.dpi": out.dpi, "axes.titlesize": 13, "axes.labelsize": 11, "legend.frameon": False})
    return out

def _fmt_pct(ax):
    ax.yaxis.set_major_formatter(lambda x, pos: f"{x:.0%}")

def plot_cum_return(spy_nav: pd.Series, composite_navs: Dict[str, pd.Series], out_path: str, cfg: PlotCfg):
    if spy_nav is None or len(composite_navs) == 0: return
    plt.figure(figsize=(9.5, 5.2))
    # Convert to cumulative return (since first point)
    spy = spy_nav / spy_nav.iloc[0] - 1.0
    plt.plot(spy.index, spy.values, color=cfg.colors["spy"], linewidth=cfg.line["width_highlight"], label="S&P (SPY)")
    for name, nav in composite_navs.items():
        cr = nav / nav.iloc[0] - 1.0
        plt.plot(cr.index, cr.values, color=cfg.colors["ikf_green"], linewidth=cfg.line["width_highlight"], label=f"IKF {name}")
        # annotate last point
        plt.text(cr.index[-1], cr.values[-1], f"  {name} {cr.values[-1]:+.1%}", color=cfg.colors["ikf_green"], va="center")
    plt.text(spy.index[-1], spy.values[-1], f"  S&P {spy.values[-1]:+.1%}", color=cfg.colors["spy"], va="center")
    _fmt_pct(plt.gca())
    plt.title("Cumulative Return")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.savefig(out_path)
    plt.close()

def plot_rolling_sharpe(spy_returns: pd.Series, comp_returns: Dict[str, pd.Series], window: int, out_path: str, cfg: PlotCfg):
    if spy_returns is None or len(comp_returns) == 0: return
    plt.figure(figsize=(9.5, 5.2))
    rs_spy = (spy_returns.rolling(window).mean() / spy_returns.rolling(window).std()) * np.sqrt(252)
    plt.plot(rs_spy.index, rs_spy.values, color=cfg.colors["spy"], linewidth=cfg.line["width_highlight"], label="S&P (SPY)")
    for name, r in comp_returns.items():
        rs = (r.rolling(window).mean() / r.rolling(window).std()) * np.sqrt(252)
        plt.plot(rs.index, rs.values, color=cfg.colors["ikf_green"], linewidth=cfg.line["width_highlight"], label=f"IKF {name}")
    plt.axhline(0, color="#999", linewidth=1, linestyle="--")
    plt.title(f"Rolling Sharpe ({window}D)")
    plt.legend(loc="best")
    plt.tight_layout()
    plt.savefig(out_path); plt.close()

def plot_drawdown(spy_nav: pd.Series, comp_navs: Dict[str, pd.Series], out_path: str, cfg: PlotCfg):
    if spy_nav is None or len(comp_navs) == 0: return
    def dd(s: pd.Series):
        return s / s.cummax() - 1.0
    plt.figure(figsize=(9.5, 5.2))
    plt.plot(spy_nav.index, dd(spy_nav).values, color=cfg.colors["spy"], linewidth=cfg.line["width_highlight"], label="S&P (SPY)")
    for name, nav in comp_navs.items():
        plt.plot(nav.index, dd(nav).values, color=cfg.colors["ikf_green"], linewidth=cfg.line["width_highlight"], label=f"IKF {name}")
    _fmt_pct(plt.gca())
    plt.title("Drawdown")
    plt.legend(loc="lower left")
    plt.tight_layout()
    plt.savefig(out_path); plt.close()

def plot_risk_return(spy_returns: pd.Series, comp_returns: Dict[str, pd.Series], out_path: str, cfg: PlotCfg):
    
...
[truncated]
```

### `modules/ranking.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/ranking.py)

```py
# modules/ranking.py
from __future__ import annotations
import re
import numpy as np
import pandas as pd

EPS = 1e-12


# ------------------------ Robust transforms ------------------------ #
def _mad(x: np.ndarray) -> float:
    med = np.nanmedian(x)
    return np.nanmedian(np.abs(x - med))


def _safe_robust_z(s: pd.Series) -> pd.Series:
    """Robust z; returns NaNs if too few valid points or constant series."""
    s = pd.to_numeric(s, errors="coerce")
    n = s.notna().sum()
    if n < 3:
        return pd.Series(np.nan, index=s.index)
    med = np.nanmedian(s.values)
    mad = _mad(s.values)
    if not np.isfinite(med) or mad == 0 or not np.isfinite(mad):
        return pd.Series(np.nan, index=s.index)
    out = (s - med) / (1.4826 * (mad + EPS))
    out = out.replace([np.inf, -np.inf], np.nan)
    return out


def winsorize(s: pd.Series, lo: float = -3.0, hi: float = 3.0) -> pd.Series:
    return s.clip(lo, hi)


def zscore(s: pd.Series) -> pd.Series:
    s = pd.to_numeric(s, errors="coerce")
    m = s.mean(skipna=True)
    sd = s.std(skipna=True)
    if not np.isfinite(sd) or sd == 0:
        return pd.Series(np.nan, index=s.index)
    return (s - m) / (sd + EPS)


# ------------------------ Neutralization ------------------------ #
def _residualize(y: pd.Series, X: pd.DataFrame) -> pd.Series:
    """Cross-sectional OLS residuals: y ~ X (safe to NaNs)."""
    X = X.copy()
    mask = y.notna()
    for c in X.columns:
        mask &= X[c].notna()
    if mask.sum() < 3:
        yc = y - y.mean(skipna=True)
        return yc.fillna(np.nan)
    yv = y[mask].values.astype(float)
    XV = X[mask].values.astype(float)
    XV = np.c_[XV, np.ones(len(XV))]
    beta = np.linalg.pinv(XV.T @ XV) @ (XV.T @ yv)
    y_hat = XV @ beta
    resid = yv - y_hat
    out = pd.Series(np.nan, index=y.index)
    out.loc[mask.index[mask]] = resid
    return out


# ------------------------ Input normalization ------------------------ #
def _normalize_input(
    features: pd.DataFrame,
    *,
    date_col: str,
    id_col: str,
    sector_col: str | None,
) -> pd.DataFrame:
    df = features.copy()

    if date_col not in df.columns:
        if isinstance(df.index, pd.DatetimeIndex):
            df = df.reset_index()
        candidates = [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]
        candidates += [date_col, "date", "Date", "datetime", "Datetime", "timestamp", "Timestamp"]
        picked = next((c for c in candidates if c in df.columns), None)
        if picked is None:
            first = df.columns[0]
            try:
                pd.to_datetime(df[first]); picked = first
            except Exception:
                pass
        if picked is None:
            raise KeyError(f"Could not find a datetime column for '{date_col}'. Columns: {list(df.columns)[:12]} ...")
        if picked != date_col:
            df = df.rename(columns={picked: date_col})
    df[date_col] = pd.to_datetime(df[date_col], errors="coerce")

    if id_col not in df.columns:
        for alt in ("Ticker", "ticker", "symbol", "Symbol"):
            if alt in df.columns:
                df = df.rename(columns={alt: id_col})
                break
    if id_col not in df.columns:
        raise KeyError(f"Missing identifier column '{id_col}'. Columns: {list(df.columns)[:12]} ...")

    if sector_col and sector_col not in df.columns:
        df[sector_col] = "ALL"

    if "ln_mcap" not in df.columns and "market_cap" in df.columns:
        df["ln_mcap"] = np.log(pd.to_numeric(df["market_cap"], errors="coerce").replace(0, np.nan))

    return df


# ------------------------ Auto-detect feature columns ------------------------ #
_ALIAS_PATTERNS: dict[str, list[tuple[str, bool]]] = {
    "momentum": [
        (r"^mom(_\d+d)?$", True),
        (r"^ret_\d+d$", True),
        (r"^momentum.*$", True),
        (r"^roc(_\d+d)?$", True),
    ],
    "volatility": [
        (r"^vol(_\d+d)?$", False),
        (r"^stdev(_\d+d)?$", False),
        (r"^atr(_\d+)
...
[truncated]
```

### `modules/rates.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/rates.py)

```py
import time
import pandas as pd
from pandas_datareader import data as pdr
from utils.helpers import parquet_path, incremental_append, load_parquet, compute_fetch_start

def update_rates(cfg: dict, storage_root: str):
    if not cfg.get("enabled", False):
        return

    series        = cfg.get("fred_series", [])
    history_start = cfg.get("start", "2010-01-01")
    overlap_days  = int(cfg.get("overlap_days", 5))

    for s in series:
        path  = parquet_path(storage_root, "rates", s)
        exist = load_parquet(path)
        start = compute_fetch_start(
            exist.index if not exist.empty else None,
            history_start=history_start,
            overlap_days=overlap_days
        )

        try:
            df = pdr.DataReader(s, "fred", start)   # index=Date, col=series
        except Exception as e:
            print(f"[WARN] {s}: download error: {e}")
            continue

        if df.empty:
            print(f"[INFO] {s}: no rows returned for start={start}")
            continue

        df.index = pd.to_datetime(df.index); df.index.name = "Date"
        incremental_append(df, path, index_name="Date")
        print(f"[OK] {s}: window={start}..today rows={len(df):,} -> {path}")

```

### `modules/report_equities.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/report_equities.py)

```py
# modules/report_equities.py
from __future__ import annotations
from pathlib import Path
import pandas as pd

def _to_md_table(df: pd.DataFrame) -> str:
    """Use pandas.to_markdown if available (tabulate installed); else fallback to code block."""
    try:
        return df.to_markdown(index=False)
    except Exception:
        # Fallback: simple code block table
        return "```\n" + df.to_string(index=False) + "\n```"

def make_equity_report(
    *,
    features_path="data/processed/equity_features.parquet",
    ranks_path="output/equity_rank_snapshot.csv",
    plots_dir="output/plots",
    out_md="output/reports/equities_report.md",
    top_k=25,
    bottom_k=25,
) -> str:
    Path(out_md).parent.mkdir(parents=True, exist_ok=True)
    Path(plots_dir).mkdir(parents=True, exist_ok=True)

    ranks = pd.read_csv(ranks_path, parse_dates=["date"])
    latest = ranks["date"].max()
    today = ranks[ranks["date"] == latest].copy()

    top = today.nlargest(top_k, "score")[["ticker", "score", "rank_pct", "decile"]]
    bot = today.nsmallest(bottom_k, "score")[["ticker", "score", "rank_pct", "decile"]]
    summary = today["score"].describe()[["mean", "std", "min", "25%", "50%", "75%", "max"]]

    md = []
    md.append(f"# IKF Equities — Composite Snapshot ({latest.date()})")
    md.append("")
    md.append("**Composite Score Meaning:** standardized cross-sectional z-score (per date). "
              "0≈average, +1≈one standard deviation above peers.")
    md.append("")
    md.append("## Summary Statistics")
    md.append(_to_md_table(summary.to_frame("value").reset_index().rename(columns={"index":"metric"})))

    md.append("\n## Top Ranked Tickers")
    md.append(_to_md_table(top))

    md.append("\n## Bottom Ranked Tickers")
    md.append(_to_md_table(bot))

    md.append("\n## Plots")
    for p in ["risk_return.png", "corr_heatmap.png", "ic_timeseries.png"]:
        f = Path(plots_dir) / p
        if f.exists():
            md.append(f"\n![{p}]({f.as_posix()})")

    Path(out_md).write_text("\n".join(md), encoding="utf-8")
    print(f"[OK] Markdown report written → {out_md}")
    return out_md

```

### `modules/reporting.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/modules/reporting.py)

```py
# modules/reporting.py
from __future__ import annotations
from pathlib import Path
from typing import List, Dict, Optional
import os
import pandas as pd

PLOT_FILES = {
    "cum_return": "equities_cum_return.png",
    "rolling_sharpe": "equities_rolling_sharpe.png",
    "drawdown": "equities_drawdown.png",
    "risk_return": "equities_risk_return.png",
    "corr_heatmap": "equities_corr_heatmap.png",
}

def _md_table(df: pd.DataFrame) -> str:
    if df is None or df.empty:
        return "_(no data)_\n"
    return df.to_markdown(index=False)

def write_equities_report(
    out_md: Path | str,
    asof: str,
    scorecard: pd.DataFrame | None,
    leaders: pd.DataFrame | None,
    laggards: pd.DataFrame | None,
    plots_dir: Path,
    include_sections: list[str],
    time_windows: list[str] | None = None,
) -> None:
    out_md = Path(out_md)
    plots_dir = Path(plots_dir)

    def _rel(rel: str) -> str:
        """relative path from report file to plot file"""
        fp = (plots_dir / rel)
        return os.path.relpath(fp, start=out_md.parent)

    def _img(rel: str) -> str:
        fp = plots_dir / rel
        return f"![{rel}]({_rel(rel)})" if fp.exists() else ""

    def _link(rel: str) -> str:
        fp = plots_dir / rel
        return f"[{rel}]({_rel(rel)})" if fp.exists() else ""

    lines: list[str] = []
    lines.append(f"# IKF Equities — Composite Snapshot ({asof})")

    # summary table
    if scorecard is not None and not scorecard.empty:
        lines.append("\n## Composite Scorecard\n")
        lines.append(_md_table(scorecard))

    # leaders/laggards
    if leaders is not None and not leaders.empty:
        lines.append("\n## Top Ranked Tickers\n")
        lines.append(_md_table(leaders))
    if laggards is not None and not laggards.empty:
        lines.append("\n## Bottom Ranked Tickers\n")
        lines.append(_md_table(laggards))

    # full-period charts (show only if file exists / section enabled)
    if "cum_return" in include_sections:
        img = _img("equities_cum_return.png")
        if img:
            lines.append("\n## Cumulative Return (Full Period)\n")
            lines.append(img)
    if "drawdown" in include_sections:
        img = _img("equities_drawdown.png")
        if img:
            lines.append("\n## Drawdown (Full Period)\n")
            lines.append(img)
    if "risk_return" in include_sections:
        img = _img("equities_risk_return.png")
        if img:
            lines.append("\n## Risk vs Return (Full Period)\n")
            lines.append(img)
    if "rolling_sharpe" in include_sections:
        img = _img("equities_rolling_sharpe.png")
        if img:
            lines.append("\n## Rolling Sharpe (Full Period)\n")
            lines.append(img)
    if "corr_heatmap" in include_sections:
        img = _img("equities_corr_heatmap.png")
        if img:
            lines.append("\n## Correlation Heatmap (1D Log Returns)\n")
            lines.append(img)

    # IKF time-window snapshots
    if time_windows:
        lines.append("\n## Time-Window Snapshots (IKF)\n")
        lines.append("_Each plot compares IKF composite(s) (green) vs SPY (grey) at the specified horizon._\n")

        for lab in time_windows:
            # Generate file paths
            cum = plots_dir / f"equities_cum_return_{lab}.png"
            dd  = plots_dir / f"equities_drawdown_{lab}.png"
            rr  = plots_dir / f"equities_risk_return_{lab}.png"
            rs  = plots_dir / f"equities_rolling_sharpe_{lab}.png"

            # Build markdown links (only if file exists)
            links = []
            if cum.exists():
                links.append(f"[Cumulative Return]({cum.as_posix()})")
            if dd.exists():
                links.append(f"[Drawdown]({dd.as_posix()})")
            if rr.exists():
                links.append(f"[Risk / Return]({rr.as_posix()})")
            if rs.exists():
                links.append(f"[Rolling Sharpe]({rs.as_posix()})")

            # Write section if anyth
...
[truncated]
```

### `quick_check.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/quick_check.py)

```py
from pathlib import Path
import pandas as pd

def info_glob(pattern):
    files = sorted(Path(pattern).glob("*.parquet"))
    rows = 0
    for f in files:
        df = pd.read_parquet(f)
        print(f"{f.name:25s}  rows={len(df):7d}  start={df.index.min()}  end={df.index.max()}")
        rows += len(df)
    print(f"TOTAL files={len(files)} rows={rows}\n")

print("== Equities ==")
info_glob("data/raw/equities")

print("== Futures ==")
info_glob("data/raw/futures")

print("== Rates ==")
info_glob("data/raw/rates")

print("== Options (AAPL calls example) ==")
for f in sorted(Path("data/raw/options/AAPL").glob("calls_*.parquet")):
    df = pd.read_parquet(f)
    print(f"{f.name:30s} rows={len(df):6d}")
    
```

### `Readme.md`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/Readme.md)

```md
# IKF Market Pipeline

A universal market data and analytics pipeline for report generation.  
Fetches and updates raw market data, engineers features, computes rankings, builds composite NAVs, and generates daily reports and plots.

---

## 🚀 Quick Start

```bash
python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

### Run the Pipeline

- **Default (full analysis)**  
  Fetches data, builds features, computes rankings, builds NAVs, and writes a Markdown report.  
  ```bash
  python run_pipeline.py
  ```

- **Data-only mode**  
  Skip analytics and just update raw data (no features/ranking/report).  
  ```bash
  python run_pipeline.py --raw-only
  ```

- **Optional flags**
  - `--recent N` → rebuild last N days (e.g., `--recent 30`)
  - `--full` → full backfill from `history_start`
  - `--asset X` → run a single asset updater (`equities`, `futures`, `rates`, `options`)
  - `--config Y` → custom config path (default `config/config.yaml`)
  - `--composites <all|none|CSV>` → control which composites to build and auto-include in universe

---

## ⚙️ Output

- `data/raw/` — incrementally updated market data  
- `data/processed/equity_features.parquet` — engineered features  
- `data/processed/composites/<NAME>_prices.parquet` — per-composite price panels  
- `data/processed/composites/<NAME>_nav.parquet` — per-composite NAV series (rebased to 100)  
- `data/processed/benchmarks/spy_nav.parquet` — SPY benchmark NAV (grey)  
- `output/equity_rank_snapshot.csv` — latest composite rankings (cross-sectional)  
- `output/equity_rank_history.parquet` — persistent rank history  
- `output/plots/*.png` — charts (full period + IKF windows)  
- `output/reports/equities_report.md` — markdown report summary

> **Branding / colors:** SPY is **grey** (`#9E9E9E`). IKF composites are **green** (`#1BAA4A`).  
> Charts show **two lines**: SPY (benchmark) vs each composite NAV.

---

## 🧩 Key Features

- **Incremental daily updates** with overlap healing  
- **Feature engineering**: momentum, volatility, RSI, SMAs/EMAs, beta, size, etc.  
- **Cross-sectional ranking & composite scoring** (z-score based, neutralization options)  
- **Composite NAV builder** from component prices (equal weight by default; config-driven)  
- **Automated reporting** with full-period visuals **+ IKF time windows** (3d/7d/14d/1m/3m/1y)  
- **Clean outputs orchestrator** that renders plots and a Markdown report

---

## 📈 IKF Windowed Reporting

The pipeline now renders **six windowed charts** matching IKF forecast horizons:

- `3d`, `7d`, `14d`, `1m`, `3m`, `1y`

For each window you get:
- **Cumulative Return** (SPY grey vs composites green)
- **Drawdown**
- **Risk vs Return**
- **Rolling Sharpe** (if enough history for the chosen RS window)

These images are saved to `output/plots/` with window suffixes, e.g.:
```
equities_cum_return_3d.png
equities_cum_return_7d.png
...
equities_drawdown_1y.png
```
and are included in the Markdown report under **“Time-Window Snapshots (IKF)”**.

---

## 🔧 Configuration

Everything important lives in `config/config.yaml`.

### Plot look & feel
```yaml
plot:
  dpi: 140
  colors:
    spy: "#9E9E9E"      # grey benchmark
    ikf_green: "#1BAA4A" # IKF green
  marker:
    size_normal: 60
    size_highlight: 110
  line:
    width_normal: 2.0
    width_highlight: 3.0
```

### Report sections & IKF windows
```yaml
report:
  top_n: 10
  include_sections:
    - leaders_laggards
    - cum_return
    - rolling_sharpe
    - drawdown
    - risk_return
    - corr_heatmap
  ikf_windows: ["3d","7d","14d","1m","3m","1y"]  # <-- override these anytime
```

### Backtest / analytics knobs
```yaml
backtest:
  rolling_sharpe_window_days: 60
  rebal_freq: "monthly"   # none|monthly|quarterly
```

### Paths
```yaml
paths:
  plots: "output/plots"
  reports: "output/reports"
  snapshots: "output/snapshots"
  processed: "data/processed"
```

---

## 🧱 Composites

Define composites in `config.yam
...
[truncated]
```

### `requirements.txt`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/requirements.txt)

```txt
# Core
pandas
numpy
pyyaml
yfinance
ta
matplotlib
seaborn
tabulate

# Parquet + data sources
pyarrow
pandas-datareader

# Optional
rich
jupyter

```

### `run_pipeline.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/run_pipeline.py)

```py
from __future__ import annotations
import argparse, sys, yaml
from pathlib import Path
import pandas as pd

from modules.equities import update_equities
from modules.options import update_options
from modules.futures import update_futures
from modules.rates import update_rates
from modules.features import run_equity_features
from modules.features_dedup import finalize_equity_features_file
from modules.ranking import compute_composite_scores, save_rank_snapshot, build_metrics_cfg_from_df
from modules.report_equities import make_equity_report
from modules.composites import build_composites_from_raw

# --- Optional clean-outputs orchestrator imports (safe even if stubs) ---
from modules.perf import (  # noqa: F401
    build_composite_nav, perf_table, drawdown_series, rolling_sharpe
)
from modules.plotting import (  # noqa: F401
    load_plot_cfg, plot_cum_return, plot_rolling_sharpe, plot_drawdown, plot_risk_return, plot_corr_heatmap
)
from modules.reporting import write_equities_report  # noqa: F401
# -----------------------------------------------------------------------

def load_config(path: str = "config/config.yaml") -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

def _parse_composite_selection(arg: str | None) -> tuple[str, set[str]]:
    if not arg or arg.strip().lower() == "all": return ("all", set())
    if arg.strip().lower() == "none": return ("none", set())
    names = {x.strip() for x in arg.split(",") if x.strip()}
    return ("some", names)

def run_clean_outputs(cfg: dict) -> None:
    """
    Build SPY (grey) vs IKF composites (green) plots and a markdown report.
    Uses NAVs from data/processed/{benchmarks,composites} and honors
    report.ikf_windows in config for time-window charts.
    Never raises.
    """
    try:
        from glob import glob
        from pathlib import Path
        import pandas as pd
        from modules.plotting import (
            load_plot_cfg, plot_cum_return, plot_rolling_sharpe,
            plot_drawdown, plot_risk_return, plot_corr_heatmap
        )
        from modules.reporting import write_equities_report
        from modules.perf import perf_table

        # --- paths ---
        paths = cfg.get("paths", {})
        plots_dir = Path(paths.get("plots", "output/plots"));       plots_dir.mkdir(parents=True, exist_ok=True)
        reports_dir = Path(paths.get("reports", "output/reports")); reports_dir.mkdir(parents=True, exist_ok=True)
        snapshots_dir = Path(paths.get("snapshots", "output/snapshots")); snapshots_dir.mkdir(parents=True, exist_ok=True)
        processed_dir = Path(paths.get("processed", "data/processed"))
        (processed_dir / "composites").mkdir(parents=True, exist_ok=True)
        composites_dir = processed_dir / "composites"

        # --- load composite NAVs ---
        comp_navs: dict[str, pd.Series] = {}
        for fp in glob(str(composites_dir / "*_nav.parquet")):
            name = Path(fp).stem.replace("_nav", "")
            s = pd.read_parquet(fp).squeeze()
            s.index = pd.to_datetime(s.index)
            comp_navs[name] = s.sort_index()

        # --- load/derive SPY NAV (benchmark only) ---
        spy_nav = None
        for cand in (processed_dir / "benchmarks" / "spy_nav.parquet",
                     processed_dir / "SPY_nav.parquet"):
            if Path(cand).exists():
                spy_nav = pd.read_parquet(cand).squeeze()
                spy_nav.index = pd.to_datetime(spy_nav.index)
                spy_nav = spy_nav.sort_index()
                break

        if spy_nav is None:
            # best-effort fallback from any prices panel if configured
            try:
                root = cfg["storage"]["root"]
                px_path = Path(root) / "equities" / "prices.parquet"
                if px_path.exists():
                    df = pd.read_parquet(px_path)
                    if "SPY" in df.columns:
                        ps = df["SPY"].dropna()
                        spy_nav = (1.0 + ps.
...
[truncated]
```

### `utils/__init__.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/__init__.py)

```py

```

### `utils/build_catalog.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/build_catalog.py)

```py
from __future__ import annotations
import os, pathlib, sys, datetime

REPO_USER = "NatanIsaacRuskin"
REPO_NAME = "IKF_Market_Pipeline"
BRANCH = os.getenv("CATALOG_BRANCH", "main")
RAW_BASE = f"https://raw.githubusercontent.com/{REPO_USER}/{REPO_NAME}/{BRANCH}/"

IGNORE_DIRS = {".git", ".github", "__pycache__", ".venv", "backups", "data/cache"}
ALLOWED_EXT = {".py", ".ipynb", ".md", ".yaml", ".yml", ".toml", ".json", ".csv", ".txt"}

def should_include(rel: pathlib.Path) -> bool:
    if any(part in IGNORE_DIRS for part in rel.parts):
        return False
    if rel.name.startswith("."):  # hide dotfiles except root configs if you want
        return False
    if rel.suffix.lower() in ALLOWED_EXT:
        return True
    return False

def human_size(n: int) -> str:
    for unit in ["B","KB","MB","GB","TB"]:
        if n < 1024: return f"{n:.0f} {unit}"
        n /= 1024
    return f"{n:.0f} PB"

def main():
    root = pathlib.Path(__file__).resolve().parents[1]
    files = []
    for p in root.rglob("*"):
        rel = p.relative_to(root)
        if p.is_file() and should_include(rel):
            files.append(p)
    files.sort(key=lambda x: str(x).lower())

    lines = []
    lines.append("# IKF Market Pipeline – Code Catalog\n")
    lines.append(f"_Branch: **{BRANCH}** | Files: **{len(files)}**_\n")
    lines.append("## Index\n")
    for p in files:
        rel = p.relative_to(root).as_posix()
        stat = p.stat()
        mtime = datetime.datetime.utcfromtimestamp(stat.st_mtime).strftime("%Y-%m-%d %H:%M:%SZ")
        size = human_size(stat.st_size)
        raw = RAW_BASE + rel
        lines.append(f"- `{rel}`  ({size}, modified {mtime} UTC) → [raw]({raw})")

    lines.append("\n---\n")
    lines.append("## File Previews\n")
    for p in files:
        rel = p.relative_to(root).as_posix()
        raw = RAW_BASE + rel
        ext = p.suffix.lower().lstrip(".")
        lang = {"yml":"yaml"}.get(ext, ext)  # nicer fencing
        try:
            text = p.read_text(encoding="utf-8", errors="replace")
        except Exception as e:
            text = f"<<could not read: {e}>>"

        # Keep the catalog small but useful (preview only)
        snippet = text if len(text) <= 4000 else text[:4000] + "\n...\n[truncated]"

        lines.append(f"\n### `{rel}`  •  [raw]({raw})\n")
        lines.append(f"```{lang}\n{snippet}\n```")

    (root / "CATALOG.md").write_text("\n".join(lines), encoding="utf-8")
    print("Wrote CATALOG.md")

if __name__ == "__main__":
    sys.exit(main())

```

### `utils/build_navs_from_prices.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/build_navs_from_prices.py)

```py
# utils/build_navs_from_prices.py
from __future__ import annotations
from pathlib import Path
import os, json, yaml, hashlib
from typing import Optional, Dict, Iterable
import pandas as pd

PANEL_PATH = Path("data/processed/equities_prices.parquet")
RAW_DIR    = Path("data/raw/equities")
BENCH_DIR  = Path("data/processed/benchmarks")
COMP_DIR   = Path("data/processed/composites")

def load_config(path: str = "config/config.yaml") -> dict:
    with open(path, "r") as f:
        return yaml.safe_load(f)

# ---------- io + math helpers ----------
def _read_existing_series(path: Path) -> Optional[pd.Series]:
    if not path.exists(): return None
    s = pd.read_parquet(path).squeeze()
    s.index = pd.to_datetime(s.index)
    return s.sort_index()

def _continue_nav(existing: Optional[pd.Series], daily_ret: pd.Series) -> pd.Series:
    daily_ret = daily_ret.sort_index()
    if existing is None or existing.empty:
        nav = (1.0 + daily_ret.fillna(0.0)).cumprod() * 100.0
        nav.name = "NAV"
        return nav
    last = existing.index.max()
    tail = daily_ret[daily_ret.index > last]
    if tail.empty:  # nothing new
        return existing
    ext = (1.0 + tail.fillna(0.0)).cumprod() * float(existing.iloc[-1])
    ext.name = "NAV"
    return pd.concat([existing, ext])

def _eqw_daily_returns(px: pd.DataFrame) -> pd.Series:
    px = px.sort_index().ffill().dropna(how="all", axis=1)
    if px.shape[1] == 0:
        return pd.Series(dtype=float, name="ret")
    rets = px.pct_change()
    return rets.mean(axis=1, skipna=True).rename("ret").dropna()

def _fingerprint(d: Dict) -> str:
    blob = json.dumps(d, sort_keys=True)
    return hashlib.sha256(blob.encode()).hexdigest()[:12]

# ---------- robust series reader ----------
_PREFER = ("Adj Close","AdjClose","adj_close","Close","close","Price","price","PX","px","Value","value","Last","last")

def _series_from_any_parquet(fp: Path, rename_to: Optional[str] = None) -> pd.Series:
    """Read parquet (Series/DF/MultiIndex), set a real datetime index, pick a price column,
    and return a single float Series named by the ticker."""
    obj = pd.read_parquet(fp)

    # If it's already a Series, try to fix index then return
    if isinstance(obj, pd.Series):
        s = obj
        idx = s.index
        # if there's a 'date' attribute/column, ignore for Series
    else:
        df = obj

        # 1) Ensure datetime index
        #    - If a 'date' column exists, use it
        #    - Else if index looks like epoch ints, parse with s/ms
        #    - Else try default to_datetime on index
        date_col = next((c for c in df.columns if str(c).lower() in ("date","datetime","asof")), None)
        if date_col is not None:
            idx = pd.to_datetime(df[date_col])
        else:
            # index might be ints; try epoch seconds then millis
            try:
                idx_try = pd.to_datetime(df.index)
                if idx_try.dtype.kind == "M":
                    idx = idx_try
                else:
                    raise ValueError
            except Exception:
                # if integer-like, try epoch s then ms
                try:
                    idx = pd.to_datetime(df.index.astype("int64"), unit="s")
                except Exception:
                    idx = pd.to_datetime(df.index.astype("int64"), unit="ms")

        df = df.copy()
        df.index = idx

        # 2) choose price column
        prefer = ("Adj Close","AdjClose","adj_close","Close","close",
                  "Price","price","PX","px","Value","value","Last","last")
        if isinstance(df.columns, pd.MultiIndex):
            target = None
            for want in prefer:
                cand = [c for c in df.columns
                        if any(str(lv).strip().lower() == want.lower()
                               for lv in (c if isinstance(c, tuple) else (c,)))]
                if cand:
                    target = cand[0]; break
            if target is None:
                # fallbac
...
[truncated]
```

### `utils/helpers.py`  •  [raw](https://raw.githubusercontent.com/NatanIsaacRuskin/IKF_Market_Pipeline/main/utils/helpers.py)

```py
from pathlib import Path
import pandas as pd

def ensure_dir(path: Path):
    """Create directory if it doesn't exist."""
    path.mkdir(parents=True, exist_ok=True)

def parquet_path(root: str, *parts: str) -> Path:
    """Build a .parquet file path under the given root."""
    dirp = Path(root, *parts[:-1])
    ensure_dir(dirp)
    stem = parts[-1]
    return dirp / f"{stem}.parquet"

def load_parquet(path: Path) -> pd.DataFrame:
    """Read parquet file if it exists, else return empty DataFrame."""
    return pd.read_parquet(path) if Path(path).exists() else pd.DataFrame()

def incremental_append(df_new: pd.DataFrame, path: Path, index_name: str = None):
    """
    Append new data to an existing parquet file without duplicates.
    Upsert semantics: concat -> sort -> drop duplicate index (keep last).
    """
    df_old = load_parquet(path)
    if df_old.empty:
        df_all = df_new.copy()
    else:
        df_all = pd.concat([df_old, df_new]).sort_index()
        df_all = df_all[~df_all.index.duplicated(keep="last")]
    if index_name:
        df_all.index.name = index_name
    df_all.to_parquet(path)

# ---------- NEW: smart fetch window ----------
def compute_fetch_start(existing_index, history_start="2010-01-01", overlap_days=5) -> str:
    """
    Decide a safe, small window to fetch:
      start = max(history_start, last_date - overlap_days), clamped to today.
    Returns ISO date string.
    """
    today = pd.Timestamp.today().normalize()
    if existing_index is None or len(existing_index) == 0:
        return pd.Timestamp(history_start).date().isoformat()
    last = pd.to_datetime(max(existing_index)).normalize()
    start = (last - pd.Timedelta(days=overlap_days)).normalize()
    start = max(pd.Timestamp(history_start), start)
    start = min(start, today)
    return start.date().isoformat()

```